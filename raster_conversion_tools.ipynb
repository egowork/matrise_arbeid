{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d9fd3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved random_geotiff_1.tif\n",
      "Saved random_geotiff_2.tif\n",
      "Saved random_geotiff_3.tif\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "# Parameters for random raster\n",
    "width, height = 100, 100\n",
    "count = 1\n",
    "dtype = 'float32'\n",
    "crs = CRS.from_epsg(25833)\n",
    "transform = from_origin(500000, 7000000, 10, 10)  # arbitrary origin and 10x10m pixels\n",
    "\n",
    "for i in range(3):\n",
    "    data = np.random.rand(height, width).astype(dtype)\n",
    "    filename = f\"random_geotiff_{i+1}.tif\"\n",
    "    with rasterio.open(\n",
    "        filename,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=count,\n",
    "        dtype=dtype,\n",
    "        crs=crs,\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(data, 1)\n",
    "    print(f\"Saved {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de90c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191abcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.shutil import copy as rio_copy\n",
    "from rasterio.features import shapes\n",
    "from rasterio.mask import mask as rasterio_mask\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, Polygon, box\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "def find_tiffs(folder):\n",
    "    \"\"\"Recursively find all .tif or .tiff files in a folder.\"\"\"\n",
    "    tiffs = []\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(('.tif', '.tiff')):\n",
    "                tiffs.append(os.path.join(root, f))\n",
    "    return tiffs\n",
    "\n",
    "def convert_to_cog(src_path, dst_path):\n",
    "    \"\"\"Convert a raster to COG with LZW compression and pyramids.\"\"\"\n",
    "    rio_copy(\n",
    "        src_path,\n",
    "        dst_path,\n",
    "        driver='COG',\n",
    "        compress='LZW',\n",
    "        overview_resampling=Resampling.nearest,\n",
    "        overview_levels=[2, 4, 8, 16]\n",
    "    )\n",
    "\n",
    "def get_raster_metadata(raster_path):\n",
    "    \"\"\"Get spatial metadata from a raster file.\"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        return {\n",
    "            'crs': src.crs,\n",
    "            'transform': src.transform,\n",
    "            'width': src.width,\n",
    "            'height': src.height,\n",
    "            'bounds': src.bounds\n",
    "        }\n",
    "\n",
    "def check_spatial_alignment(raster_path, reference_meta):\n",
    "    \"\"\"Check if raster aligns with reference metadata.\"\"\"\n",
    "    current_meta = get_raster_metadata(raster_path)\n",
    "    \n",
    "    alignment_check = (\n",
    "        current_meta['crs'] == reference_meta['crs'] and\n",
    "        current_meta['transform'] == reference_meta['transform'] and\n",
    "        current_meta['width'] == reference_meta['width'] and\n",
    "        current_meta['height'] == reference_meta['height']\n",
    "    )\n",
    "    \n",
    "    return alignment_check, current_meta\n",
    "\n",
    "def geometries_almost_equal(geom1, geom2, tolerance=1e-6):\n",
    "    \"\"\"Check if two geometries are approximately equal within a tolerance.\"\"\"\n",
    "    try:\n",
    "        # Check if geometries are exactly equal first\n",
    "        if geom1.equals(geom2):\n",
    "            return True\n",
    "        \n",
    "        # Check if they have the same type\n",
    "        if type(geom1) != type(geom2):\n",
    "            return False\n",
    "        \n",
    "        # Check if centroids are close\n",
    "        centroid_distance = geom1.centroid.distance(geom2.centroid)\n",
    "        if centroid_distance > tolerance:\n",
    "            return False\n",
    "        \n",
    "        # Check if areas are similar (within 1% tolerance)\n",
    "        area_diff = abs(geom1.area - geom2.area) / max(geom1.area, geom2.area, 1e-10)\n",
    "        if area_diff > 0.01:\n",
    "            return False\n",
    "        \n",
    "        # Check if they have high intersection overlap\n",
    "        if geom1.intersects(geom2):\n",
    "            intersection_area = geom1.intersection(geom2).area\n",
    "            overlap_ratio = intersection_area / min(geom1.area, geom2.area)\n",
    "            return overlap_ratio > 0.95\n",
    "        \n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def create_pixel_polygons(raster_data, transform, mask_array):\n",
    "    \"\"\"\n",
    "    Create individual polygon for each valid pixel to ensure consistent spatial structure.\n",
    "    \n",
    "    This prevents merging of adjacent pixels with same values, which is crucial for\n",
    "    incremental processing where subsequent rasters may have different value distributions.\n",
    "    \n",
    "    Optimized version using vectorized operations and shapely.box for better performance.\n",
    "    \"\"\"\n",
    "    height, width = raster_data.shape\n",
    "    pixel_width = abs(transform.a)\n",
    "    pixel_height = abs(transform.e)\n",
    "    \n",
    "    # Find valid (unmasked) pixel coordinates\n",
    "    valid_rows, valid_cols = np.where(mask_array)\n",
    "    \n",
    "    if len(valid_rows) == 0:\n",
    "        return [], [], []\n",
    "    \n",
    "    # Vectorized coordinate calculations\n",
    "    x_mins = transform.c + valid_cols * transform.a\n",
    "    y_maxs = transform.f + valid_rows * transform.e\n",
    "    \n",
    "    # Optimized polygon creation using shapely.box (faster for rectangles)\n",
    "    geometries = [\n",
    "        box(x_min, y_max - pixel_height, x_min + pixel_width, y_max)\n",
    "        for x_min, y_max in zip(x_mins, y_maxs)\n",
    "    ]\n",
    "    \n",
    "    # Vectorized value and index extraction\n",
    "    values = raster_data[valid_rows, valid_cols].astype(float).tolist()\n",
    "    pixel_indices = (valid_rows * width + valid_cols).tolist()\n",
    "    \n",
    "    return geometries, values, pixel_indices\n",
    "\n",
    "def raster_to_vector_ordered(raster_path, mask_value=None, simplify_tolerance=None, source_filename=None):\n",
    "    \"\"\"\n",
    "    Convert raster to vector polygons maintaining consistent polygon order.\n",
    "    \n",
    "    Creates individual polygons for each pixel to ensure consistent spatial structure\n",
    "    across aligned rasters. This prevents merging of adjacent pixels with same values.\n",
    "    \n",
    "    Returns polygons in the same order as raster values are read (row-major order).\n",
    "    This ensures consistent ordering for aligned rasters.\n",
    "    \"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        raster_data = src.read(1)\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "        nodata = src.nodata\n",
    "    \n",
    "    # Create mask efficiently\n",
    "    if mask_value is not None:\n",
    "        mask_array = raster_data != mask_value\n",
    "    elif nodata is not None:\n",
    "        mask_array = raster_data != nodata\n",
    "    else:\n",
    "        mask_array = np.ones_like(raster_data, dtype=bool)\n",
    "    \n",
    "    # Check for NoData and issue warning for incremental processing\n",
    "    masked_pixels = np.sum(~mask_array)\n",
    "    total_pixels = raster_data.size\n",
    "    if masked_pixels > 0:\n",
    "        print(f\"  Warning: {masked_pixels}/{total_pixels} pixels masked as NoData in {source_filename}\")\n",
    "    \n",
    "    # Create individual pixel polygons to prevent merging\n",
    "    geometries, values, pixel_indices = create_pixel_polygons(raster_data, transform, mask_array)\n",
    "    \n",
    "    if not geometries:\n",
    "        return gpd.GeoDataFrame(columns=['geometry'], crs=crs)\n",
    "    \n",
    "    # Create GeoDataFrame - already in row-major order\n",
    "    gdf_data = {\n",
    "        'geometry': geometries,\n",
    "        'value': values,\n",
    "        'pixel_index': pixel_indices  # Keep for debugging/verification\n",
    "    }\n",
    "    \n",
    "    if source_filename:\n",
    "        gdf_data['source_file'] = source_filename\n",
    "    \n",
    "    gdf = gpd.GeoDataFrame(gdf_data, crs=crs)\n",
    "    \n",
    "    print(f\"  Created {len(gdf)} individual pixel polygons (no merging)\")\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "def extract_raster_values_ordered(raster_path, base_gdf, mask_value=None):\n",
    "    \"\"\"\n",
    "    Extract values from raster for each polygon in base_gdf, maintaining order.\n",
    "    \n",
    "    For spatially aligned rasters, this is much faster than spatial matching.\n",
    "    Uses pixel indices for precise correspondence.\n",
    "    \"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        raster_data = src.read(1)\n",
    "        transform = src.transform\n",
    "        nodata = src.nodata\n",
    "    \n",
    "    values = []\n",
    "    \n",
    "    # Use pixel_index if available for precise mapping\n",
    "    if 'pixel_index' in base_gdf.columns:\n",
    "        height, width = raster_data.shape\n",
    "        \n",
    "        for pixel_idx in base_gdf['pixel_index']:\n",
    "            row = pixel_idx // width\n",
    "            col = pixel_idx % width\n",
    "            \n",
    "            if 0 <= row < height and 0 <= col < width:\n",
    "                value = float(raster_data[row, col])\n",
    "                # Check for mask values\n",
    "                if mask_value is not None and value == mask_value:\n",
    "                    values.append(np.nan)\n",
    "                elif nodata is not None and value == nodata:\n",
    "                    values.append(np.nan)\n",
    "                else:\n",
    "                    values.append(value)\n",
    "            else:\n",
    "                values.append(np.nan)\n",
    "    else:\n",
    "        # Fallback to centroid method\n",
    "        for geom in base_gdf.geometry:\n",
    "            try:\n",
    "                # Get centroid and sample\n",
    "                centroid = geom.centroid\n",
    "                col = int((centroid.x - transform.c) / transform.a)\n",
    "                row = int((centroid.y - transform.f) / transform.e)\n",
    "                \n",
    "                # Ensure coordinates are within bounds\n",
    "                if 0 <= row < raster_data.shape[0] and 0 <= col < raster_data.shape[1]:\n",
    "                    value = float(raster_data[row, col])\n",
    "                    # Check for mask value\n",
    "                    if mask_value is not None and value == mask_value:\n",
    "                        values.append(np.nan)\n",
    "                    elif nodata is not None and value == nodata:\n",
    "                        values.append(np.nan)\n",
    "                    else:\n",
    "                        values.append(value)\n",
    "                else:\n",
    "                    values.append(np.nan)\n",
    "            except Exception:\n",
    "                values.append(np.nan)\n",
    "    \n",
    "    return values\n",
    "\n",
    "def raster_to_vector(raster_path, output_path=None, mask_value=None, simplify_tolerance=None, source_filename=None):\n",
    "    \"\"\"\n",
    "    Optimized conversion of raster to vector polygons.\n",
    "    \n",
    "    Parameters:\n",
    "    - raster_path: Path to raster file or numpy array\n",
    "    - output_path: Output path for vector file (optional)\n",
    "    - mask_value: Value to mask out\n",
    "    - simplify_tolerance: Tolerance for polygon simplification (meters)\n",
    "    - source_filename: Name of source file to add as attribute\n",
    "    \n",
    "    Returns:\n",
    "    - GeoDataFrame with polygons\n",
    "    \"\"\"\n",
    "    if isinstance(raster_path, (str, os.PathLike)):\n",
    "        # Read from file with optimized reading\n",
    "        with rasterio.open(raster_path) as src:\n",
    "            # Read all bands at once if multiband\n",
    "            if src.count > 1:\n",
    "                raster_data = src.read()  # Shape: (bands, height, width)\n",
    "                is_multiband = True\n",
    "                reference_band = raster_data[0]\n",
    "            else:\n",
    "                raster_data = src.read(1)\n",
    "                is_multiband = False\n",
    "                reference_band = raster_data\n",
    "            \n",
    "            transform = src.transform\n",
    "            crs = src.crs\n",
    "            \n",
    "    elif isinstance(raster_path, np.ndarray):\n",
    "        raster_data = raster_path\n",
    "        is_multiband = raster_data.ndim == 3\n",
    "        \n",
    "        if is_multiband:\n",
    "            reference_band = raster_data[0]\n",
    "        else:\n",
    "            reference_band = raster_data\n",
    "            \n",
    "        # Default transform and CRS for array input\n",
    "        transform = rasterio.transform.from_bounds(0, 0, reference_band.shape[1], reference_band.shape[0], \n",
    "                                                 reference_band.shape[1], reference_band.shape[0])\n",
    "        crs = 'EPSG:4326'\n",
    "    else:\n",
    "        raise ValueError(\"raster_path must be a file path or numpy array\")\n",
    "    \n",
    "    # Create mask efficiently\n",
    "    if mask_value is not None:\n",
    "        mask_array = reference_band != mask_value\n",
    "    else:\n",
    "        mask_array = np.ones_like(reference_band, dtype=bool)\n",
    "    \n",
    "    # Extract shapes with connectivity for better polygon extraction\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        polygon_generator = shapes(reference_band, mask=mask_array, transform=transform, connectivity=8)\n",
    "    \n",
    "    # Process polygons more efficiently\n",
    "    geometries = []\n",
    "    polygon_values = []\n",
    "    \n",
    "    for geom_dict, value in polygon_generator:\n",
    "        geom = shape(geom_dict)\n",
    "        \n",
    "        # Optional simplification for performance\n",
    "        if simplify_tolerance and hasattr(geom, 'simplify'):\n",
    "            geom = geom.simplify(simplify_tolerance, preserve_topology=True)\n",
    "        \n",
    "        geometries.append(geom)\n",
    "        polygon_values.append(value)\n",
    "    \n",
    "    if not geometries:\n",
    "        # Return empty GeoDataFrame if no polygons found\n",
    "        return gpd.GeoDataFrame(columns=['geometry'], crs=crs)\n",
    "    \n",
    "    # Create GeoDataFrame more efficiently\n",
    "    if is_multiband:\n",
    "        # Optimized multi-band value extraction using vectorized operations\n",
    "        gdf_data = {'geometry': geometries}\n",
    "        \n",
    "        # Use rasterio.mask for efficient value extraction per polygon\n",
    "        for band_idx in range(raster_data.shape[0]):\n",
    "            band_values = []\n",
    "            current_band = raster_data[band_idx] if is_multiband else raster_data\n",
    "            \n",
    "            # For small datasets, use centroid method (faster)\n",
    "            # For large datasets, consider using rasterio.mask (more accurate but slower)\n",
    "            if len(geometries) < 10000:  # Threshold for method selection\n",
    "                for geom in geometries:\n",
    "                    try:\n",
    "                        # Get centroid and sample\n",
    "                        centroid = geom.centroid\n",
    "                        col = int((centroid.x - transform.c) / transform.a)\n",
    "                        row = int((centroid.y - transform.f) / transform.e)\n",
    "                        \n",
    "                        if 0 <= row < current_band.shape[0] and 0 <= col < current_band.shape[1]:\n",
    "                            band_values.append(float(current_band[row, col]))\n",
    "                        else:\n",
    "                            band_values.append(np.nan)\n",
    "                    except Exception:\n",
    "                        band_values.append(np.nan)\n",
    "            else:\n",
    "                # For larger datasets, use mean value within polygon\n",
    "                for geom in geometries:\n",
    "                    try:\n",
    "                        # Create temporary dataset for masking\n",
    "                        temp_transform = transform\n",
    "                        temp_shape = current_band.shape\n",
    "                        \n",
    "                        # Use rasterio mask to get values within polygon\n",
    "                        masked_data, _ = rasterio_mask(\n",
    "                            [{'data': current_band, 'transform': temp_transform}],\n",
    "                            [geom],\n",
    "                            crop=True,\n",
    "                            nodata=np.nan\n",
    "                        )\n",
    "                        \n",
    "                        if masked_data.size > 0:\n",
    "                            band_values.append(float(np.nanmean(masked_data)))\n",
    "                        else:\n",
    "                            band_values.append(np.nan)\n",
    "                    except Exception:\n",
    "                        band_values.append(np.nan)\n",
    "            \n",
    "            gdf_data[f'band_{band_idx + 1}'] = band_values\n",
    "        \n",
    "        gdf = gpd.GeoDataFrame(gdf_data, crs=crs)\n",
    "    else:\n",
    "        # Single band case\n",
    "        gdf = gpd.GeoDataFrame({\n",
    "            'value': polygon_values,\n",
    "            'geometry': geometries\n",
    "        }, crs=crs)\n",
    "    \n",
    "    # Add source filename if provided\n",
    "    if source_filename:\n",
    "        gdf['source_file'] = source_filename\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if output_path:\n",
    "        # Use optimized driver options\n",
    "        if output_path.lower().endswith('.gpkg'):\n",
    "            gdf.to_file(output_path, driver='GPKG')\n",
    "        else:\n",
    "            gdf.to_file(output_path)\n",
    "        print(f\"Vector file saved to: {output_path}\")\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "def map_attributes_to_existing_features(base_gdf, new_gdf, source_name):\n",
    "    \"\"\"\n",
    "    Map attributes from new GeoDataFrame to spatially matching features in base GeoDataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_gdf: Existing GeoDataFrame with base features\n",
    "    - new_gdf: New GeoDataFrame with additional attributes to map\n",
    "    - source_name: Name to use for the new attribute column\n",
    "    \n",
    "    Returns:\n",
    "    - Updated base GeoDataFrame with new attributes\n",
    "    \"\"\"\n",
    "    if new_gdf.empty:\n",
    "        # Add empty column if no new data\n",
    "        base_gdf[f'{source_name}_value'] = np.nan\n",
    "        return base_gdf\n",
    "    \n",
    "    # Create spatial index for efficient lookups\n",
    "    base_sindex = base_gdf.sindex\n",
    "    \n",
    "    # Initialize new attribute column\n",
    "    new_values = np.full(len(base_gdf), np.nan)\n",
    "    \n",
    "    # For each new feature, find spatially matching base features\n",
    "    for new_idx, new_row in new_gdf.iterrows():\n",
    "        new_geom = new_row.geometry\n",
    "        new_value = new_row.get('value', new_row.get('band_1', np.nan))\n",
    "        \n",
    "        # Find potential matches using spatial index\n",
    "        possible_matches_idx = list(base_sindex.intersection(new_geom.bounds))\n",
    "        \n",
    "        if possible_matches_idx:\n",
    "            # Check for exact spatial matches\n",
    "            for base_idx in possible_matches_idx:\n",
    "                base_geom = base_gdf.iloc[base_idx].geometry\n",
    "                \n",
    "                # Check if geometries are approximately equal using custom function\n",
    "                if geometries_almost_equal(new_geom, base_geom):\n",
    "                    new_values[base_idx] = new_value\n",
    "                    break\n",
    "                # Alternative: check for significant overlap\n",
    "                elif new_geom.intersects(base_geom):\n",
    "                    intersection_area = new_geom.intersection(base_geom).area\n",
    "                    overlap_ratio = intersection_area / min(new_geom.area, base_geom.area)\n",
    "                    if overlap_ratio > 0.9:  # 90% overlap threshold\n",
    "                        new_values[base_idx] = new_value\n",
    "                        break\n",
    "    \n",
    "    # Add new attribute column\n",
    "    base_gdf[f'{source_name}_value'] = new_values\n",
    "    \n",
    "    # Count successful mappings\n",
    "    mapped_count = np.sum(~np.isnan(new_values))\n",
    "    total_new = len(new_gdf)\n",
    "    \n",
    "    print(f\"  Mapped {mapped_count}/{total_new} features from {source_name}\")\n",
    "    \n",
    "    return base_gdf\n",
    "\n",
    "def stack_rasters(raster_paths, check_alignment=True):\n",
    "    \"\"\"Optimized raster stacking with optional alignment checking.\"\"\"\n",
    "    if not raster_paths:\n",
    "        raise ValueError(\"No raster paths provided\")\n",
    "    \n",
    "    # Read first raster to get reference properties\n",
    "    with rasterio.open(raster_paths[0]) as ref_src:\n",
    "        ref_meta = {\n",
    "            'crs': ref_src.crs,\n",
    "            'transform': ref_src.transform,\n",
    "            'width': ref_src.width,\n",
    "            'height': ref_src.height,\n",
    "            'dtype': ref_src.dtypes[0]\n",
    "        }\n",
    "        \n",
    "        # Pre-allocate array for better memory efficiency\n",
    "        stack_array = np.empty((len(raster_paths), ref_src.height, ref_src.width), dtype=ref_meta['dtype'])\n",
    "        stack_array[0] = ref_src.read(1)\n",
    "    \n",
    "    # Read remaining rasters\n",
    "    for i, path in enumerate(raster_paths[1:], 1):\n",
    "        with rasterio.open(path) as src:\n",
    "            if check_alignment:\n",
    "                if (src.crs != ref_meta['crs'] or \n",
    "                    src.transform != ref_meta['transform'] or \n",
    "                    src.width != ref_meta['width'] or \n",
    "                    src.height != ref_meta['height']):\n",
    "                    raise ValueError(f\"Raster {path} does not align spatially with reference raster\")\n",
    "            \n",
    "            stack_array[i] = src.read(1)\n",
    "    \n",
    "    return stack_array\n",
    "\n",
    "def get_cog_path(tif_path):\n",
    "    \"\"\"Generate COG path for a given TIFF file.\"\"\"\n",
    "    return os.path.splitext(tif_path)[0] + '_cog.tif'\n",
    "\n",
    "def prefer_cog_files(tiff_files):\n",
    "    \"\"\"\n",
    "    Efficiently prefer COG versions using set operations.\n",
    "    \"\"\"\n",
    "    # Create sets for faster lookup\n",
    "    all_files_set = set(tiff_files)\n",
    "    preferred_files = []\n",
    "    processed = set()\n",
    "    \n",
    "    for tif in tiff_files:\n",
    "        if tif in processed:\n",
    "            continue\n",
    "            \n",
    "        if tif.endswith('_cog.tif'):\n",
    "            preferred_files.append(tif)\n",
    "            processed.add(tif)\n",
    "        else:\n",
    "            cog_path = get_cog_path(tif)\n",
    "            if cog_path in all_files_set:\n",
    "                preferred_files.append(cog_path)\n",
    "                processed.add(cog_path)\n",
    "            else:\n",
    "                preferred_files.append(tif)\n",
    "            processed.add(tif)\n",
    "    \n",
    "    return preferred_files\n",
    "\n",
    "def incremental_vector_processing(tiff_files, vector_output_path, mask_value=None, \n",
    "                                simplify_tolerance=None, combine_features=True):\n",
    "    \"\"\"\n",
    "    Process rasters incrementally to avoid memory issues.\n",
    "    \n",
    "    For spatially aligned rasters, uses optimized value concatenation instead of spatial matching.\n",
    "    Creates individual pixel polygons to ensure consistent spatial structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - tiff_files: List of TIFF file paths\n",
    "    - vector_output_path: Output path for vector file\n",
    "    - mask_value: Value to mask out\n",
    "    - simplify_tolerance: Tolerance for polygon simplification\n",
    "    - combine_features: If True, combine all features into one file; if False, create separate files\n",
    "    \n",
    "    Returns:\n",
    "    - List of GeoDataFrames or single combined GeoDataFrame\n",
    "    \"\"\"\n",
    "    gdfs = []\n",
    "    combined_gdf = None\n",
    "    reference_meta = None\n",
    "    all_spatially_aligned = True\n",
    "    \n",
    "    # Check spatial alignment for all files first\n",
    "    if combine_features and len(tiff_files) > 1:\n",
    "        print(\"Checking spatial alignment of all rasters...\")\n",
    "        reference_meta = get_raster_metadata(tiff_files[0])\n",
    "        \n",
    "        for i, tif in enumerate(tiff_files[1:], 1):\n",
    "            source_name = os.path.splitext(os.path.basename(tif))[0]\n",
    "            is_aligned, current_meta = check_spatial_alignment(tif, reference_meta)\n",
    "            if not is_aligned:\n",
    "                all_spatially_aligned = False\n",
    "                print(f\"  Warning: {source_name} does not align spatially - using slower spatial matching\")\n",
    "                break\n",
    "        \n",
    "        if all_spatially_aligned:\n",
    "            print(\"  All rasters are spatially aligned - using optimized processing\")\n",
    "            print(\"  Creating individual pixel polygons to prevent value merging\")\n",
    "    \n",
    "    for i, tif in enumerate(tiff_files):\n",
    "        print(f\"Processing raster {i+1}/{len(tiff_files)}: {os.path.basename(tif)}\")\n",
    "        \n",
    "        # Get source filename for attribution\n",
    "        source_name = os.path.splitext(os.path.basename(tif))[0]\n",
    "        \n",
    "        if combine_features:\n",
    "            if i == 0:\n",
    "                # First raster becomes the base - use ordered conversion for consistency\n",
    "                print(f\"  Creating base vector layer from: {source_name}\")\n",
    "                combined_gdf = raster_to_vector_ordered(tif, mask_value=mask_value, \n",
    "                                                      simplify_tolerance=simplify_tolerance,\n",
    "                                                      source_filename=source_name)\n",
    "                \n",
    "                if combined_gdf.empty:\n",
    "                    print(f\"  No polygons found in {source_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Rename value column to include source name\n",
    "                if 'value' in combined_gdf.columns:\n",
    "                    combined_gdf.rename(columns={'value': f'{source_name}_value'}, inplace=True)\n",
    "                \n",
    "                print(f\"  Base layer created with {len(combined_gdf)} polygons\")\n",
    "                \n",
    "            else:\n",
    "                # Subsequent rasters\n",
    "                if all_spatially_aligned:\n",
    "                    # Fast path: directly extract values for aligned rasters\n",
    "                    print(f\"  Extracting values from aligned raster: {source_name}\")\n",
    "                    new_values = extract_raster_values_ordered(tif, combined_gdf, mask_value)\n",
    "                    combined_gdf[f'{source_name}_value'] = new_values\n",
    "                    \n",
    "                    # Count non-NaN values\n",
    "                    valid_count = np.sum(~np.isnan(new_values))\n",
    "                    print(f\"  Added {valid_count}/{len(new_values)} values from {source_name}\")\n",
    "                else:\n",
    "                    # Slow path: spatial matching for non-aligned rasters\n",
    "                    print(f\"  Using spatial matching for: {source_name}\")\n",
    "                    gdf = raster_to_vector(tif, mask_value=mask_value, \n",
    "                                         simplify_tolerance=simplify_tolerance,\n",
    "                                         source_filename=source_name)\n",
    "                    \n",
    "                    if gdf.empty:\n",
    "                        print(f\"  No polygons found in {source_name}\")\n",
    "                        combined_gdf[f'{source_name}_value'] = np.nan\n",
    "                        continue\n",
    "                    \n",
    "                    # Map attributes from new raster to existing features\n",
    "                    combined_gdf = map_attributes_to_existing_features(combined_gdf, gdf, source_name)\n",
    "        else:\n",
    "            # Keep separate - use standard conversion\n",
    "            gdf = raster_to_vector(tif, mask_value=mask_value, \n",
    "                                 simplify_tolerance=simplify_tolerance,\n",
    "                                 source_filename=source_name)\n",
    "            \n",
    "            if gdf.empty:\n",
    "                print(f\"  No polygons found in {source_name}\")\n",
    "                continue\n",
    "            \n",
    "            gdfs.append(gdf)\n",
    "            \n",
    "            # Save individual file\n",
    "            if vector_output_path:\n",
    "                base_name = os.path.splitext(vector_output_path)[0]\n",
    "                ext = os.path.splitext(vector_output_path)[1] or '.gpkg'\n",
    "                individual_output = f\"{base_name}_{source_name}{ext}\"\n",
    "                gdf.to_file(individual_output)\n",
    "                print(f\"  Saved {len(gdf)} polygons to {individual_output}\")\n",
    "    \n",
    "    if combine_features and combined_gdf is not None:\n",
    "        # Clean up and save combined file\n",
    "        # Remove helper columns before saving\n",
    "        columns_to_drop = ['source_file']\n",
    "        for col in columns_to_drop:\n",
    "            if col in combined_gdf.columns:\n",
    "                combined_gdf = combined_gdf.drop(columns=[col])\n",
    "        \n",
    "        if vector_output_path:\n",
    "            if vector_output_path.lower().endswith('.gpkg'):\n",
    "                combined_gdf.to_file(vector_output_path, driver='GPKG')\n",
    "            else:\n",
    "                combined_gdf.to_file(vector_output_path)\n",
    "            print(f\"Combined vector file saved to: {vector_output_path}\")\n",
    "            \n",
    "            # Print summary of attributes\n",
    "            value_columns = [col for col in combined_gdf.columns if col.endswith('_value')]\n",
    "            print(f\"Final dataset contains {len(combined_gdf)} features with {len(value_columns)} value attributes\")\n",
    "            \n",
    "            if all_spatially_aligned:\n",
    "                print(\"Processing completed using optimized aligned raster method with individual pixel polygons\")\n",
    "        \n",
    "        return combined_gdf\n",
    "    \n",
    "    return gdfs if not combine_features else combined_gdf\n",
    "\n",
    "def process_folder(folder, convert_cog=False, stack=False, to_vector=False, \n",
    "                   vector_output_path=None, mask_value=None, simplify_tolerance=None,\n",
    "                   parallel_cog=False, check_alignment=True, combine_features=True,\n",
    "                   incremental_processing=False):\n",
    "    \"\"\"\n",
    "    Optimized folder processing with additional performance options.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder: Folder to search for TIFF files\n",
    "    - convert_cog: Whether to convert TIFFs to COG format\n",
    "    - stack: Whether to stack aligned rasters\n",
    "    - to_vector: Whether to convert raster(s) to vector polygons\n",
    "    - vector_output_path: Output path for vector file (if to_vector=True)\n",
    "    - mask_value: Value to mask out when converting to vector\n",
    "    - simplify_tolerance: Polygon simplification tolerance for faster processing\n",
    "    - parallel_cog: Use parallel processing for COG conversion\n",
    "    - check_alignment: Whether to check raster alignment (disable for speed if certain)\n",
    "    - combine_features: If True, combine all vector features into one file/GDF; if False, keep separate\n",
    "    - incremental_processing: If True, process rasters one by one to avoid memory issues\n",
    "    \n",
    "    Returns:\n",
    "    - List of file paths, stacked array, GeoDataFrame, or list of GeoDataFrames depending on options\n",
    "    \"\"\"\n",
    "    # Find all TIFF files\n",
    "    all_tiffs = find_tiffs(folder)\n",
    "    \n",
    "    if not all_tiffs:\n",
    "        print(\"No TIFF files found\")\n",
    "        return []\n",
    "    \n",
    "    # Always prefer COG versions if they exist\n",
    "    tiffs = prefer_cog_files(all_tiffs)\n",
    "    \n",
    "    if convert_cog:\n",
    "        files_to_convert = []\n",
    "        converted_files = []\n",
    "        \n",
    "        for tif in tiffs:\n",
    "            if tif.endswith('_cog.tif'):\n",
    "                converted_files.append(tif)\n",
    "                continue\n",
    "                \n",
    "            cog_path = get_cog_path(tif)\n",
    "            \n",
    "            if not os.path.exists(cog_path):\n",
    "                files_to_convert.append((tif, cog_path))\n",
    "            else:\n",
    "                print(f\"COG already exists for {tif}, skipping conversion.\")\n",
    "            \n",
    "            converted_files.append(cog_path)\n",
    "        \n",
    "        # Convert files (optionally in parallel)\n",
    "        if files_to_convert:\n",
    "            if parallel_cog and len(files_to_convert) > 1:\n",
    "                print(f\"Converting {len(files_to_convert)} files to COG in parallel...\")\n",
    "                with ThreadPoolExecutor(max_workers=min(16, len(files_to_convert))) as executor:\n",
    "                    executor.map(lambda x: convert_to_cog(x[0], x[1]), files_to_convert)\n",
    "            else:\n",
    "                for src, dst in files_to_convert:\n",
    "                    print(f\"Converting {src} to COG...\")\n",
    "                    convert_to_cog(src, dst)\n",
    "        \n",
    "        tiffs = converted_files\n",
    "    \n",
    "    # Handle vector processing\n",
    "    if to_vector:\n",
    "        if incremental_processing or (not stack and len(tiffs) > 1):\n",
    "            # Use incremental processing for memory efficiency\n",
    "            print(\"Using incremental processing to avoid memory issues...\")\n",
    "            return incremental_vector_processing(\n",
    "                tiffs, vector_output_path, mask_value, \n",
    "                simplify_tolerance, combine_features\n",
    "            )\n",
    "        elif stack:\n",
    "            # Stack first, then vectorize\n",
    "            print(\"Stacking rasters first, then vectorizing...\")\n",
    "            stacked_data = stack_rasters(tiffs, check_alignment=check_alignment)\n",
    "            return raster_to_vector(stacked_data, vector_output_path, mask_value, simplify_tolerance)\n",
    "        else:\n",
    "            # Single file\n",
    "            return raster_to_vector(tiffs[0], vector_output_path, mask_value, simplify_tolerance)\n",
    "    \n",
    "    # Handle stacking without vectorization\n",
    "    if stack:\n",
    "        return stack_rasters(tiffs, check_alignment=check_alignment)\n",
    "    \n",
    "    return tiffs\n",
    "\n",
    "# Alias for backwards compatibility\n",
    "#process_folder = process_folder_optimized\n",
    "#raster_to_vector = raster_to_vector_optimized\n",
    "#stack_rasters = stack_rasters_optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9496fd3",
   "metadata": {},
   "source": [
    "# make find_tiffs() run in current folder\n",
    "if __name__ == \"__main__\":\n",
    "    folder = \".\"\n",
    "    print(\"Finding TIFF files...\")\n",
    "    tiff_files = find_tiffs(folder)\n",
    "    print(f\"Found {len(tiff_files)} TIFF files.\")\n",
    "\n",
    "    print(\"Converting to COG...\")\n",
    "    process_folder(folder, convert_cog=True)\n",
    "\n",
    "    print(\"Stacking aligned rasters...\")\n",
    "    try:\n",
    "        stacked_data = process_folder(folder, stack=True)\n",
    "        print(f\"Stacked data shape: {stacked_data.shape}\")\n",
    "        \n",
    "        print(\"Converting stacked raster to vector...\")\n",
    "        vector_gdf = process_folder(folder, stack=False, to_vector=True, \n",
    "                                  vector_output_path=\"stacked_polygons.shp\")\n",
    "        print(f\"Created {len(vector_gdf)} polygons\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e568ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = r\"G:\\arcgis_prosjekt_publisert_azure\\servicedata\\naturskog_v1\\project_9ae905e7-a7a5-43ed-b468-63b29232d4b8\\delivery_6baba287-82a7-4c32-b481-fa1a864b046a\"\n",
    "folder = r\"C:\\Users\\endofs\\Downloads\\project_9ae905e7-a7a5-43ed-b468-63b29232d4b8\\delivery_6baba287-82a7-4c32-b481-fa1a864b046a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580d33e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using incremental processing to avoid memory issues...\n",
      "Checking spatial alignment of all rasters...\n",
      "  All rasters are spatially aligned - using optimized processing\n",
      "  Creating individual pixel polygons to prevent value merging\n",
      "Processing raster 1/3: random_geotiff_1_cog.tif\n",
      "  Creating base vector layer from: random_geotiff_1_cog\n",
      "  Created 10000 individual pixel polygons (no merging)\n",
      "  Base layer created with 10000 polygons\n",
      "Processing raster 2/3: random_geotiff_2_cog.tif\n",
      "  Extracting values from aligned raster: random_geotiff_2_cog\n",
      "  Added 10000/10000 values from random_geotiff_2_cog\n",
      "Processing raster 3/3: random_geotiff_3_cog.tif\n",
      "  Extracting values from aligned raster: random_geotiff_3_cog\n",
      "  Added 10000/10000 values from random_geotiff_3_cog\n",
      "Combined vector file saved to: combined_analysis.gpkg\n",
      "Final dataset contains 10000 features with 3 value attributes\n",
      "Processing completed using optimized aligned raster method with individual pixel polygons\n",
      "  Created 10000 individual pixel polygons (no merging)\n",
      "  Base layer created with 10000 polygons\n",
      "Processing raster 2/3: random_geotiff_2_cog.tif\n",
      "  Extracting values from aligned raster: random_geotiff_2_cog\n",
      "  Added 10000/10000 values from random_geotiff_2_cog\n",
      "Processing raster 3/3: random_geotiff_3_cog.tif\n",
      "  Extracting values from aligned raster: random_geotiff_3_cog\n",
      "  Added 10000/10000 values from random_geotiff_3_cog\n",
      "Combined vector file saved to: combined_analysis.gpkg\n",
      "Final dataset contains 10000 features with 3 value attributes\n",
      "Processing completed using optimized aligned raster method with individual pixel polygons\n"
     ]
    }
   ],
   "source": [
    "# Example usage with combined features\n",
    "result = process_folder(\n",
    "    folder=\".\",\n",
    "    to_vector=True,\n",
    "    combine_features=True,  # Maps attributes instead of concatenating\n",
    "    incremental_processing=True,\n",
    "    vector_output_path=\"combined_analysis.gpkg\"\n",
    ")\n",
    "\n",
    "# Result will have:\n",
    "# - Same number of features as first raster\n",
    "# - Columns: geometry, raster1_value, raster2_value, raster3_value, etc.\n",
    "# - Spatial alignment enforced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54898e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using incremental processing to avoid memory issues...\n",
      "Checking spatial alignment of all rasters...\n",
      "  All rasters are spatially aligned - using optimized processing\n",
      "  Creating individual pixel polygons to prevent value merging\n",
      "Processing raster 1/6: naturskog_binaer_cog.tif\n",
      "  Creating base vector layer from: naturskog_binaer_cog\n",
      "  All rasters are spatially aligned - using optimized processing\n",
      "  Creating individual pixel polygons to prevent value merging\n",
      "Processing raster 1/6: naturskog_binaer_cog.tif\n",
      "  Creating base vector layer from: naturskog_binaer_cog\n",
      "  Warning: 6457931329/6909003504 pixels masked as NoData in naturskog_binaer_cog\n",
      "  Warning: 6457931329/6909003504 pixels masked as NoData in naturskog_binaer_cog\n"
     ]
    }
   ],
   "source": [
    "# Example usage with combined features\n",
    "folder = r\"C:\\Users\\endofs\\Downloads\\project_9ae905e7-a7a5-43ed-b468-63b29232d4b8\\delivery_6baba287-82a7-4c32-b481-fa1a864b046a\"\n",
    "\n",
    "result = process_folder(\n",
    "    folder=folder,\n",
    "    convert_cog=True, # Convert to COG if not already, especcially beneficial for large rasters\n",
    "    parallel_cog=True, # Use parallel processing for COG conversion\n",
    "    to_vector=True, # Convert to vector polygons\n",
    "    combine_features=True,  # Maps attributes instead of concatenating\n",
    "    incremental_processing=True, # to avoid memory issues\n",
    "    vector_output_path=\"naturskog_v1.gpkg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d62528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear all variables from memory\n",
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matrise-arbeid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
